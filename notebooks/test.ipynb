{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6c0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sample_collection, label_field, labels_path, classes, label_type='detections', include_missing=False)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import fiftyone.utils.yolo as fouy\n",
    "\n",
    "print(inspect.signature(fouy.add_yolo_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d888814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "fo.launch_app(auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc139e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Total VRAM: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea2085c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>split</th>\n",
       "      <th>label_id</th>\n",
       "      <th>category</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>scene</th>\n",
       "      <th>timeofday</th>\n",
       "      <th>weather</th>\n",
       "      <th>traffic_light_color</th>\n",
       "      <th>occluded</th>\n",
       "      <th>truncated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b1c66a42-6f7d68ca.jpg</td>\n",
       "      <td>val</td>\n",
       "      <td>0</td>\n",
       "      <td>traffic sign</td>\n",
       "      <td>1000.698742</td>\n",
       "      <td>281.992415</td>\n",
       "      <td>1040.626872</td>\n",
       "      <td>326.911560</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>city street</td>\n",
       "      <td>daytime</td>\n",
       "      <td>overcast</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b1c66a42-6f7d68ca.jpg</td>\n",
       "      <td>val</td>\n",
       "      <td>1</td>\n",
       "      <td>traffic sign</td>\n",
       "      <td>214.613695</td>\n",
       "      <td>172.190058</td>\n",
       "      <td>274.505889</td>\n",
       "      <td>229.586743</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>city street</td>\n",
       "      <td>daytime</td>\n",
       "      <td>overcast</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b1c66a42-6f7d68ca.jpg</td>\n",
       "      <td>val</td>\n",
       "      <td>2</td>\n",
       "      <td>traffic sign</td>\n",
       "      <td>797.314833</td>\n",
       "      <td>313.186265</td>\n",
       "      <td>829.756437</td>\n",
       "      <td>341.884608</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>city street</td>\n",
       "      <td>daytime</td>\n",
       "      <td>overcast</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b1c66a42-6f7d68ca.jpg</td>\n",
       "      <td>val</td>\n",
       "      <td>3</td>\n",
       "      <td>traffic sign</td>\n",
       "      <td>652.575363</td>\n",
       "      <td>303.204232</td>\n",
       "      <td>685.016968</td>\n",
       "      <td>315.681772</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>city street</td>\n",
       "      <td>daytime</td>\n",
       "      <td>overcast</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1c66a42-6f7d68ca.jpg</td>\n",
       "      <td>val</td>\n",
       "      <td>4</td>\n",
       "      <td>traffic light</td>\n",
       "      <td>707.476543</td>\n",
       "      <td>311.938510</td>\n",
       "      <td>716.210821</td>\n",
       "      <td>328.159313</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>city street</td>\n",
       "      <td>daytime</td>\n",
       "      <td>overcast</td>\n",
       "      <td>green</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_name split  label_id       category           x1  \\\n",
       "0  b1c66a42-6f7d68ca.jpg   val         0   traffic sign  1000.698742   \n",
       "1  b1c66a42-6f7d68ca.jpg   val         1   traffic sign   214.613695   \n",
       "2  b1c66a42-6f7d68ca.jpg   val         2   traffic sign   797.314833   \n",
       "3  b1c66a42-6f7d68ca.jpg   val         3   traffic sign   652.575363   \n",
       "4  b1c66a42-6f7d68ca.jpg   val         4  traffic light   707.476543   \n",
       "\n",
       "           y1           x2          y2  width  height        scene timeofday  \\\n",
       "0  281.992415  1040.626872  326.911560   1280     720  city street   daytime   \n",
       "1  172.190058   274.505889  229.586743   1280     720  city street   daytime   \n",
       "2  313.186265   829.756437  341.884608   1280     720  city street   daytime   \n",
       "3  303.204232   685.016968  315.681772   1280     720  city street   daytime   \n",
       "4  311.938510   716.210821  328.159313   1280     720  city street   daytime   \n",
       "\n",
       "    weather traffic_light_color  occluded  truncated  \n",
       "0  overcast                none     False      False  \n",
       "1  overcast                none     False      False  \n",
       "2  overcast                none     False      False  \n",
       "3  overcast                none     False      False  \n",
       "4  overcast               green     False      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"../results/parsed_data/val_data.parquet\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "model = torch.hub.load('hustvl/YOLOP', 'yolop', pretrained=True)  \n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef6d2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model1 = YOLO(\"../runs/train/yolo11s_NoFreeze/weights/best.pt\")\n",
    "model2 = YOLO(\"../models/yolov8l_bdd.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e32c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'rider', 2: 'car', 3: 'bus', 4: 'truck', 5: 'bike', 6: 'motor', 7: 'traffic light', 8: 'traffic sign', 9: 'train'}\n",
      "{0: 'pedestrian', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motorcycle', 7: 'bicycle', 8: 'traffic light', 9: 'traffic sign'}\n"
     ]
    }
   ],
   "source": [
    "class_names_1 = model1.names\n",
    "class_names_2 = model2.names\n",
    "\n",
    "print(class_names_1)\n",
    "print(class_names_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28265a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.162 ðŸš€ Python-3.12.3 torch-2.7.1+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 112 layers, 43,614,318 parameters, 0 gradients, 164.9 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1132.6Â±387.0 MB/s, size: 41.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/anish/data/projects/autonomous-vision-bdd100k/data/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [03:14<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     185526      0.371      0.262      0.244      0.135\n",
      "            pedestrian       3220      13262      0.604      0.574      0.592      0.294\n",
      "                 rider        515        649      0.449      0.381       0.36      0.178\n",
      "                   car       9879     102506      0.698      0.746      0.772      0.484\n",
      "                 truck       1242       1597      0.124      0.357     0.0992      0.064\n",
      "                   bus       2689       4245      0.305      0.142       0.23      0.146\n",
      "                 train        578       1007          1          0          0          0\n",
      "            motorcycle        334        452       0.52      0.283      0.318      0.146\n",
      "               bicycle       5653      26885          0          0   1.53e-05   3.07e-06\n",
      "         traffic light       8221      34908       0.01    0.00647     0.0706      0.036\n",
      "          traffic sign         14         15   6.45e-05      0.133   1.79e-05   9.32e-06\n",
      "Speed: 0.1ms preprocess, 14.0ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1moutputs/detect/val3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = model2.val(data=\"../configs/datasets/_bdd10_auto.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35e10093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.162 ðŸš€ Python-3.12.3 torch-2.7.1+cu126 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11s summary (fused): 100 layers, 9,416,670 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 67.9Â±20.6 MB/s, size: 63.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/anish/data/projects/autonomous-vision-bdd100k/data/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:41<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     185526      0.674      0.436      0.478      0.269\n",
      "                person       3220      13262      0.678      0.513      0.574      0.281\n",
      "                 rider        515        649      0.566      0.361      0.386      0.183\n",
      "                   car       9879     102506      0.752       0.71      0.763      0.476\n",
      "                   bus       1242       1597      0.606       0.52      0.563      0.432\n",
      "                 truck       2689       4245      0.627      0.546      0.595      0.427\n",
      "                  bike        578       1007       0.54      0.362      0.381      0.187\n",
      "                 motor        334        452      0.597      0.279      0.342       0.17\n",
      "         traffic light       5653      26885      0.687      0.512      0.565      0.212\n",
      "          traffic sign       8221      34908       0.69      0.553      0.606      0.319\n",
      "                 train         14         15          1          0          0          0\n",
      "Speed: 0.1ms preprocess, 2.9ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Saving outputs/detect/val5/predictions.json...\n",
      "Results saved to \u001b[1moutputs/detect/val5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = model1.val(data=\"../configs/datasets/_bdd10_auto.yaml\", save_json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aa0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.yolo as fouy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1393b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"bdd10_auto_val\"\n",
    "\n",
    "if fo.dataset_exists(DATASET_NAME):\n",
    "    fo.delete_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c5c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"bdd100k-val-ultralytics-pred\"\n",
    "\n",
    "if fo.dataset_exists(DATASET_NAME):\n",
    "    fo.delete_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d66e0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "if fo.dataset_exists(\"test-yolo-dataset\"):\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c5e140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"test-yolo-dataset\"\n",
    "dataset_dir = \"../data\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.YOLOv5Dataset, \n",
    "    name=name,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28ffb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        test-yolo-dataset\n",
      "Media type:  image\n",
      "Num samples: 10000\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "[<Sample: {\n",
      "    'id': '689c6415f943268c3325a6d6',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/home/anish/data/projects/autonomous-vision-bdd100k/data/images/val/b1c66a42-6f7d68ca.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': datetime.datetime(2025, 8, 13, 10, 8, 21, 621000),\n",
      "    'last_modified_at': datetime.datetime(2025, 8, 13, 10, 8, 21, 621000),\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6b4',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.781796, 0.391656, 0.031194, 0.062388],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6b5',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.1676665, 0.23915299999999998, 0.046791, 0.079718],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6b6',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.6229025, 0.4349805, 0.025345, 0.039859],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6b7',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.5098245, 0.421117, 0.025345, 0.01733],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6b8',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.552716, 0.4332475, 0.006824, 0.022529],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6b9',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.489354, 0.41071850000000004, 0.007798, 0.029461],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6ba',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.24760100000000002, 0.402054, 0.009748, 0.024262],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6bb',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.2115335, 0.4003215, 0.030219, 0.019063],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6bc',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [\n",
      "                    0.17254150000000001,\n",
      "                    0.41765050000000004,\n",
      "                    0.008773,\n",
      "                    0.015597,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6bd',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.1608435, 0.469641, 0.059463, 0.06932],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6be',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [\n",
      "                    0.03704249999999999,\n",
      "                    0.47830550000000005,\n",
      "                    0.064337,\n",
      "                    0.077985,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6bf',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.1930115, 0.47830549999999994, 0.077985, 0.071053],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c0',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.0, 0.4679075, 0.040942, 0.091849],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c1',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'rider',\n",
      "                'bounding_box': [0.5078745, 0.49217000000000005, 0.010723, 0.038126],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c2',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'motor',\n",
      "                'bounding_box': [0.5069, 0.5094995, 0.011698, 0.029461],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c3',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.5566154999999999, 0.4679075, 0.010723, 0.022529],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c4',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.534195, 0.4956355, 0.029244, 0.050257],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c5',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.5517409999999999, 0.5060335, 0.021446, 0.036393],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c6',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [\n",
      "                    0.5683130000000001,\n",
      "                    0.5077659999999999,\n",
      "                    0.02632,\n",
      "                    0.048524,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c7',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.5848845, 0.502567, 0.046791, 0.065854],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c8',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.6151035, 0.4973679999999999, 0.092607, 0.093582],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6c9',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.6872389999999999, 0.521631, 0.062388, 0.105712],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6ca',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.7311055, 0.466175, 0.215433, 0.206226],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6cb',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [\n",
      "                    0.9406899999999999,\n",
      "                    0.5770865000000001,\n",
      "                    0.058488,\n",
      "                    0.159435,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6cc',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.4708325, 0.473107, 0.005849, 0.01733],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6cd',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.462059, 0.4557765, 0.004874, 0.019063],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6ce',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.4133185, 0.460976, 0.005849, 0.013864],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6cf',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [\n",
      "                    0.43086549999999996,\n",
      "                    0.49390300000000004,\n",
      "                    0.010723,\n",
      "                    0.01733,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d0',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.44841200000000003, 0.488704, 0.009748, 0.024262],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d1',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.45426049999999996, 0.4956355, 0.012673, 0.029461],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d2',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.465958, 0.48870349999999996, 0.031194, 0.046791],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d3',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.4045455, 0.49390300000000004, 0.013647, 0.01733],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d4',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.39187249999999996, 0.4956355, 0.012673, 0.019063],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d5',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.3538555, 0.490437, 0.020471, 0.027728],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "}>, <Sample: {\n",
      "    'id': '689c6415f943268c3325a735',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/home/anish/data/projects/autonomous-vision-bdd100k/data/images/val/b1c81faa-3df17267.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': datetime.datetime(2025, 8, 13, 10, 8, 21, 630000),\n",
      "    'last_modified_at': datetime.datetime(2025, 8, 13, 10, 8, 21, 630000),\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d7',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.6402059999999999, 0.3890035, 0.05451, 0.045361],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d8',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.802578, 0.403437, 0.019716, 0.02268],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6d9',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.829253, 0.3250855, 0.05683, 0.043299],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6da',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic light',\n",
      "                'bounding_box': [0.785181, 0.366323, 0.019716, 0.02268],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "}>, <Sample: {\n",
      "    'id': '689c6415f943268c3325a736',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/home/anish/data/projects/autonomous-vision-bdd100k/data/images/val/b1c81faa-c80764c5.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "    'created_at': datetime.datetime(2025, 8, 13, 10, 8, 21, 633000),\n",
      "    'last_modified_at': datetime.datetime(2025, 8, 13, 10, 8, 21, 633000),\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6db',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.506437, 0.209612, 0.023862, 0.01622],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6dc',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.39203950000000004, 0.23207, 0.067375, 0.078604],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6dd',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'traffic sign',\n",
      "                'bounding_box': [0.45871300000000004, 0.222089, 0.070884, 0.088586],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6de',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.3693355, 0.381169, 0.011931, 0.017468],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6df',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.3917945, 0.377426, 0.018247, 0.022458],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e0',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.4170595, 0.383664, 0.010527, 0.01622],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e1',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.44022, 0.38615950000000004, 0.008422, 0.012477],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e2',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.45144850000000003, 0.3886555, 0.014037, 0.011229],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e3',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.5574245, 0.3911505, 0.011229, 0.013725],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e4',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.5475995, 0.39115, 0.011229, 0.017468],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e5',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [\n",
      "                    0.5251399999999999,\n",
      "                    0.38740749999999996,\n",
      "                    0.017546,\n",
      "                    0.018715,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e6',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.49285700000000005, 0.3911505, 0.016142, 0.019963],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e7',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [\n",
      "                    0.47811849999999995,\n",
      "                    0.39115049999999996,\n",
      "                    0.026669,\n",
      "                    0.034935,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "            <Detection: {\n",
      "                'id': '689c6415f943268c3325a6e8',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'car',\n",
      "                'bounding_box': [0.2788, 0.3674445, 0.071586, 0.084843],\n",
      "                'mask': None,\n",
      "                'mask_path': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "}>]\n"
     ]
    }
   ],
   "source": [
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6db63992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_sample_tags()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d907a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"../models/yolo11s_bdd.pt\")\n",
    "dataset.apply_model(model, label_field=\"YOLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019782d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6165adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.core.labels as fol\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_bdd100k_json_to_fiftyone(\n",
    "    json_path,\n",
    "    images_dir,\n",
    "    dataset_name=\"bdd100k\",\n",
    "    detection_classes=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts BDD100K JSON annotations to FiftyOne format with selected fields.\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): Path to BDD100K JSON file.\n",
    "        images_dir (str): Directory containing the images.\n",
    "        dataset_name (str): Name for the FiftyOne dataset.\n",
    "        detection_classes (List[str], optional): Filter for object classes.\n",
    "    \n",
    "    Returns:\n",
    "        fo.Dataset: FiftyOne dataset loaded with ground truth detections and attributes.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for item in tqdm(data, desc=f\"Loading {dataset_name}\"):\n",
    "        filepath = os.path.join(images_dir, item[\"name\"])\n",
    "        labels = item.get(\"labels\", [])\n",
    "        detections = []\n",
    "\n",
    "        for obj in labels:\n",
    "            cat = obj.get(\"category\")\n",
    "            box = obj.get(\"box2d\")\n",
    "            if not box or (detection_classes and cat not in detection_classes):\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            attributes = obj.get(\"attributes\", {})\n",
    "            detection = fol.Detection(\n",
    "                label=cat,\n",
    "                bounding_box=[x1 / 1280, y1 / 720, w / 1280, h / 720],\n",
    "                attributes={\n",
    "                    \"occluded\": fol.Attribute(value=attributes.get(\"occluded\", False)),\n",
    "                    \"truncated\": fol.Attribute(value=attributes.get(\"truncated\", False)),\n",
    "                },\n",
    "            )\n",
    "            detections.append(detection)\n",
    "\n",
    "        img_attrs = item.get(\"attributes\", {})\n",
    "        sample = fo.Sample(\n",
    "            filepath=filepath,\n",
    "            ground_truth=fol.Detections(detections=detections),\n",
    "            scene=img_attrs.get(\"scene\", \"unknown\"),\n",
    "            weather=img_attrs.get(\"weather\", \"unknown\"),\n",
    "            timeofday=img_attrs.get(\"timeofday\", \"unknown\"),\n",
    "        )\n",
    "\n",
    "        samples.append(sample)\n",
    "\n",
    "    dataset = fo.Dataset(dataset_name, overwrite=True)\n",
    "    dataset.add_samples(samples)\n",
    "    dataset.compute_metadata()\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4059186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.96s)\n",
      "creating index...\n",
      "index created!\n",
      " predictions image_id values:\n",
      "fe1f55fa-19ba3600 <class 'str'>\n",
      "fe1f55fa-19ba3600 <class 'str'>\n",
      "fe1f55fa-19ba3600 <class 'str'>\n",
      "fe1f55fa-19ba3600 <class 'str'>\n",
      "fe1f55fa-19ba3600 <class 'str'>\n",
      " COCO GT image entries:\n",
      "1 b1c66a42-6f7d68ca.jpg <class 'int'>\n",
      "2 b1c81faa-3df17267.jpg <class 'int'>\n",
      "3 b1c81faa-c80764c5.jpg <class 'int'>\n",
      "4 b1c9c847-3bda4659.jpg <class 'int'>\n",
      "5 b1ca2e5d-84cf9134.jpg <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "gt_path = \"../data/coco_data/bdd100k_val_coco.json\"\n",
    "meta_path = \"../data/parsed_data/val_data.parquet\"\n",
    "\n",
    "coco_gt = COCO(gt_path)\n",
    "val_df = pd.read_parquet(meta_path)\n",
    "\n",
    "with open(\"outputs/detect/val/predictions.json\", \"r\") as f:\n",
    "    preds = json.load(f)\n",
    "\n",
    "print(\" predictions image_id values:\")\n",
    "for p in preds[:5]:\n",
    "    print(p['image_id'], type(p['image_id']))\n",
    "\n",
    "\n",
    "print(\" COCO GT image entries:\")\n",
    "for img in coco_gt.dataset['images'][:5]:\n",
    "    print(img['id'], img['file_name'], type(img['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7577cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=8.13s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "file_to_id = {img['file_name'].replace('.jpg', ''): img['id'] for img in coco_gt.dataset['images']}\n",
    "\n",
    "# Update predictions to use numeric IDs\n",
    "for p in preds:\n",
    "    p['image_id'] = file_to_id[p['image_id']]\n",
    "\n",
    "# Now we can load into COCOeval\n",
    "coco_dt = coco_gt.loadRes(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d01a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'int'>}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(set(type(p['image_id']) for p in preds)) \n",
    "\n",
    "pred_ids = {p['image_id'] for p in preds}\n",
    "gt_ids = set(coco_gt.getImgIds())\n",
    "print(len(pred_ids & gt_ids) == len(pred_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
